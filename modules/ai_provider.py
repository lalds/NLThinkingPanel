"""
–ú–æ–¥—É–ª—å AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∑–∞–ø—Ä–æ—Å–æ–≤.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç retry logic, fallback –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤.
"""
import time
from typing import Optional, Dict, Any, List
from openai import OpenAI
from core.logger import logger
from core.cache import cache
from config.config import config


class AIProvider:
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ OpenRouter."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ OpenAI."""
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=config.openrouter_api_key,
        )
        
        # Fallback –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
        self.fallback_models = [
            'anthropic/claude-3-haiku',
            'openai/gpt-3.5-turbo',
            'google/gemini-flash-1.5',
            'meta-llama/llama-3-8b-instruct:free'
        ]
    
    def generate_response(
        self,
        system_prompt: str,
        user_message: str,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
        
        Args:
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            model: –ú–æ–¥–µ–ª—å (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ config)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏: content, model, tokens_used, response_time, from_cache
        """
        start_time = time.time()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        model = model or config.openrouter_model
        temperature = temperature if temperature is not None else config.temperature
        max_tokens = max_tokens or config.max_tokens
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if use_cache and config.cache_enabled:
            cached = cache.get(system_prompt, user_message, model)
            if cached:
                logger.info(f"–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∏–∑ –∫—ç—à–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model}")
                cached['from_cache'] = True
                cached['response_time'] = time.time() - start_time
                return cached
        
        # –ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å retry logic
        models_to_try = [model] + [m for m in self.fallback_models if m != model]
        last_error = None
        
        for attempt, current_model in enumerate(models_to_try):
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –º–æ–¥–µ–ª—å {current_model}")
                
                completion = self.client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "https://github.com/NLThinkingPanel",
                        "X-Title": "NLThinkingPanel Pro",
                    },
                    model=current_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_message},
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                
                response_time = time.time() - start_time
                
                result = {
                    'content': completion.choices[0].message.content,
                    'model': current_model,
                    'tokens_used': completion.usage.total_tokens if completion.usage else 0,
                    'response_time': response_time,
                    'from_cache': False
                }
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
                if use_cache and config.cache_enabled:
                    cache.set(result, system_prompt, user_message, model)
                
                logger.info(
                    f"–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç {current_model} "
                    f"({result['tokens_used']} —Ç–æ–∫–µ–Ω–æ–≤, {response_time:.2f}s)"
                )
                
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"–û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {current_model}: {e}")
                
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                if attempt < len(models_to_try) - 1:
                    logger.info(f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ fallback –º–æ–¥–µ–ª—å...")
                    time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ retry
                    continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
        raise Exception(f"–í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
    
    def optimize_prompt(self, prompt: str, max_length: int = 4000) -> str:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤.
        
        Args:
            prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        if len(prompt) <= max_length:
            return prompt
        
        logger.warning(f"–ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è...")
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –æ–±—Ä–µ–∑–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç–µ–π
        lines = prompt.split('\n')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü (–æ–±—ã—á–Ω–æ —Ç–∞–º –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
        important_lines = []
        current_length = 0
        
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        for line in lines[:10]:
            if current_length + len(line) < max_length // 2:
                important_lines.append(line)
                current_length += len(line)
            else:
                break
        
        important_lines.append("\n... [–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â—ë–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏] ...\n")
        
        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏
        for line in reversed(lines[-10:]):
            if current_length + len(line) < max_length:
                important_lines.insert(-1, line)
                current_length += len(line)
            else:
                break
        
        optimized = '\n'.join(important_lines)
        logger.info(f"–ü—Ä–æ–º–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(prompt)} -> {len(optimized)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return optimized
    
    def estimate_tokens(self, text: str) -> int:
        """
        –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: ~4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        
        Returns:
            –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        return len(text) // 4

    def check_search_necessity(self, query: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—à–µ–≤—É—é/–±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        """
        if not query:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã (–±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å)
        quick_triggers = ["–ø–æ–≥–æ–¥–∞", "–∫—É—Ä—Å", "–Ω–æ–≤–æ—Å—Ç–∏", "—Å–µ–π—á–∞—Å", "—Å–µ–≥–æ–¥–Ω—è", "url", "http", "–ø–æ–∏—Å–∫"]
        if any(t in query.lower() for t in quick_triggers):
            return True

        system_prompt = (
            "–¢—ã ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ Google/–ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. "
            "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û 'YES', –µ—Å–ª–∏ –ø–æ–∏—Å–∫ –ù–£–ñ–ï–ù. –û—Ç–≤–µ—á–∞–π 'NO', –µ—Å–ª–∏ –º–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è.\n"
            "–ü—Ä–∏–º–µ—Ä—ã YES: '–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞?', '–∫—É—Ä—Å –±–∏—Ç–∫–æ–∏–Ω–∞', '–Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è', '–∫—Ç–æ –ø–æ–±–µ–¥–∏–ª –≤ –º–∞—Ç—á–µ', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ library v5'.\n"
            "–ü—Ä–∏–º–µ—Ä—ã NO: '–ø—Ä–∏–≤–µ—Ç', '–Ω–∞–ø–∏—à–∏ –∫–æ–¥ –Ω–∞ python', '—Ä–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç', '—á—Ç–æ —Ç–∞–∫–æ–µ –∏–Ω—Ñ–ª—è—Ü–∏—è', '–ø–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç'.\n"
            "Reply with YES or NO only."
        )
        
        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –¥–µ—à–µ–≤—É—é/–±–µ—Å–ø–ª–∞—Ç–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            classifier_model = 'google/gemini-2.0-flash-lite-preview-02-05:free'
            
            # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ fallback, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –¥–ª—è reliability
            
            res = self.client.chat.completions.create(
                model=classifier_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Query: {query}"}
                ],
                max_tokens=5,
                temperature=0.1,
                extra_headers={
                    "HTTP-Referer": "https://github.com/NLThinkingPanel",
                    "X-Title": "NLThinkingPanel Classifier",
                }
            )
            
            content = res.choices[0].message.content.strip().upper()
            logger.info(f"üîç Auto-Web Check '{query[:20]}...': {content}")
            return 'YES' in content
            
        except Exception:
            # Fallback: –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–∫—É—â—É—é —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                res = self.client.chat.completions.create(
                    model=config.openrouter_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"Query: {query}"}
                    ],
                    max_tokens=5,
                    temperature=0.1
                )
                content = res.choices[0].message.content.strip().upper()
                return 'YES' in content
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞: {e}")
                return False


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
ai_provider = AIProvider()
