"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç DuckDuckGo –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –±–∞–∑–æ–≤—ã–π scraping –¥–ª—è —Å–±–æ—Ä–∞ —Ñ–∞–∫—Ç–æ–≤.
"""
import re
import html
import socket
from urllib.parse import urlparse
from urllib.request import Request, urlopen
from typing import List, Dict, Optional

from ddgs import DDGS

from core.logger import logger
from core.cache import cache


class SearchEngine:
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–∞."""

    def __init__(self, max_results: int = 5, request_timeout: int = 10):
        self.max_results = max_results
        self.request_timeout = request_timeout

    def search(self, query: str, max_results: Optional[int] = None) -> List[Dict[str, str]]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –≤–µ–±-—Å–µ—Ç–∏."""
        limit = max_results or self.max_results

        cache_key = f"search_{query}_{limit}"
        cached_results = cache.get(cache_key)
        if cached_results:
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}' –≤–∑—è—Ç—ã –∏–∑ –∫—ç—à–∞")
            return cached_results

        try:
            logger.info(f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {query}")
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=limit))
                cache.set(results, cache_key, ttl=1800)
                return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def fetch_page_text(self, url: str, max_chars: int = 6000) -> str:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–µ—ë –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç."""
        cache_key = f"page_text_{url}_{max_chars}"
        cached_text = cache.get(cache_key)
        if cached_text:
            return cached_text

        try:
            request = Request(
                url,
                headers={
                    "User-Agent": (
                        "Mozilla/5.0 (X11; Linux x86_64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/124.0 Safari/537.36"
                    )
                }
            )

            with urlopen(request, timeout=self.request_timeout) as response:
                content_type = response.headers.get("Content-Type", "")
                if "text/html" not in content_type and "application/xhtml+xml" not in content_type:
                    return ""

                raw_html = response.read().decode("utf-8", errors="ignore")

            text = self._extract_text_from_html(raw_html)
            text = re.sub(r"\s+", " ", text).strip()
            text = text[:max_chars]

            cache.set(text, cache_key, ttl=1800)
            return text

        except (TimeoutError, socket.timeout):
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
            return ""
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url}: {e}")
            return ""

    def scrape_search_results(
        self,
        results: List[Dict[str, str]],
        max_pages: int = 3,
        per_page_chars: int = 4000
    ) -> List[Dict[str, str]]:
        """–û—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç."""
        scraped: List[Dict[str, str]] = []

        for result in results:
            if len(scraped) >= max_pages:
                break

            url = result.get("href", "")
            title = result.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            snippet = result.get("body", "")
            if not url or not url.startswith(("http://", "https://")):
                continue

            page_text = self.fetch_page_text(url, max_chars=per_page_chars)
            if not page_text:
                continue

            scraped.append(
                {
                    "title": title,
                    "href": url,
                    "domain": urlparse(url).netloc,
                    "snippet": snippet,
                    "content": page_text,
                }
            )

        return scraped

    def should_use_web_search(self, question: str, mode: str = "auto", triggers: Optional[List[str]] = None) -> bool:
        """–†–µ—à–∞–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ –≤–µ–±-–ø–æ–∏—Å–∫ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞."""
        normalized_mode = str(mode or "auto").lower().strip()
        if normalized_mode == "always":
            return True
        if normalized_mode == "off":
            return False

        q = question.lower().strip()
        if not q:
            return False

        quick_signals = ["http://", "https://", "—Å—Å—ã–ª–∫–∞", "–∏—Å—Ç–æ—á–Ω–∏–∫", "–ø—Ä—É—Ñ", "–ø–æ—Å–ª–µ–¥–Ω–∏–µ", "—Å–µ–≥–æ–¥–Ω—è", "—Å–µ–π—á–∞—Å"]
        if any(signal in q for signal in quick_signals):
            return True

        trigger_words = triggers or []
        return any(word in q for word in trigger_words)

    def gather_web_context(
        self,
        question: str,
        max_results: int = 7,
        max_pages: int = 3,
        per_page_chars: int = 3500
    ) -> Dict[str, object]:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–æ–∏—Å–∫ -> —Å–∫—Ä–∞–ø–∏–Ω–≥ -> —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        try:
            search_results = self.search(question, max_results=max_results)
        except TypeError:
            # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ —Å—Ç–∞—Ä—ã–º–∏ —Å–∏–≥–Ω–∞—Ç—É—Ä–∞–º–∏ –ø–æ—Å–ª–µ merge-–∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
            search_results = self.search(question)

        scraped_pages: List[Dict[str, str]] = []
        if search_results:
            try:
                scraped_pages = self.scrape_search_results(
                    search_results,
                    max_pages=max_pages,
                    per_page_chars=per_page_chars
                )
            except TypeError:
                scraped_pages = self.scrape_search_results(search_results)

        source_urls = [page["href"] for page in scraped_pages[:5]]
        if not source_urls:
            source_urls = [res.get("href", "") for res in search_results[:3] if res.get("href")]

        return {
            "search_results": search_results,
            "scraped_pages": scraped_pages,
            "web_context": self.format_results_for_ai(search_results),
            "scraped_context": self.format_scraped_for_ai(scraped_pages),
            "memory_summary": self.build_memory_summary(question, scraped_pages),
            "source_urls": source_urls,
        }

    def format_results_for_ai(self, results: List[Dict[str, str]]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
        if not results:
            return "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç."

        formatted = ["üåê **–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê –í –°–ï–¢–ò:**"]
        for i, res in enumerate(results, 1):
            formatted.append(f"{i}. **{res.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}**")
            formatted.append(f"   –°—Å—ã–ª–∫–∞: {res.get('href', '-')}")
            formatted.append(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {res.get('body', '')}\n")

        return "\n".join(formatted)

    def format_scraped_for_ai(self, scraped_pages: List[Dict[str, str]], max_chars_total: int = 12000) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–∫—Ä–∞–ø–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è AI."""
        if not scraped_pages:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü."

        parts: List[str] = ["üìö **–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï –°–û –°–¢–†–ê–ù–ò–¶:**"]
        used = 0

        for i, page in enumerate(scraped_pages, 1):
            block = (
                f"\n[{i}] {page['title']}\n"
                f"URL: {page['href']}\n"
                f"–î–æ–º–µ–Ω: {page['domain']}\n"
                f"–°–Ω–∏–ø–ø–µ—Ç: {page['snippet']}\n"
                f"–¢–µ–∫—Å—Ç: {page['content'][:3500]}\n"
            )

            if used + len(block) > max_chars_total:
                break

            parts.append(block)
            used += len(block)

        return "\n".join(parts)

    def build_memory_summary(self, query: str, scraped_pages: List[Dict[str, str]], max_points: int = 5) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–∞."""
        if not scraped_pages:
            return f"–ó–∞–ø—Ä–æ—Å: {query}. –°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å."

        lines = [f"–ó–∞–ø—Ä–æ—Å: {query}", "–ö–ª—é—á–µ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:"]
        for i, page in enumerate(scraped_pages[:max_points], 1):
            short_text = page["content"][:240].replace("\n", " ")
            lines.append(
                f"{i}) {page['title']} ({page['domain']}) ‚Äî {short_text}..."
            )

        return "\n".join(lines)

    def _extract_text_from_html(self, raw_html: str) -> str:
        """–ì—Ä—É–±–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–¥–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        cleaned = re.sub(r"<script[\s\S]*?</script>", " ", raw_html, flags=re.IGNORECASE)
        cleaned = re.sub(r"<style[\s\S]*?</style>", " ", cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r"<noscript[\s\S]*?</noscript>", " ", cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r"<[^>]+>", " ", cleaned)
        cleaned = html.unescape(cleaned)
        return cleaned


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
search_engine = SearchEngine()
